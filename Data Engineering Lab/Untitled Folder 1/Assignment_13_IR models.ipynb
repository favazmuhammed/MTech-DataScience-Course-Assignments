{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "588d43ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import log, sqrt\n",
    "import nltk\n",
    "from nltk.corpus import inaugural\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4074d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     C:\\Users\\favas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\favas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('inaugural')\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()               #initializing lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516f5cf8",
   "metadata": {},
   "source": [
    "#### Model-1: IR systems based on TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028f6f63",
   "metadata": {},
   "source": [
    "###### Rank by TF-IDF score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28733d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for pre-processing the test\n",
    "# this function can be used with TfidfVectorizer\n",
    "def preprocess(x):\n",
    "    words = word_tokenize(x)                                 #text to tokens\n",
    "    words = [word for word in words if word.isalnum()]       #remove non alphanumeric\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]   #lemmetize\n",
    "    words = [lemmatizer.lemmatize(word, pos='a') for word in words] #lemetize adjectives\n",
    "    return ' '.join(words)                                          #join back to single string and return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bdbea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function calculate total tfidf for a document based on query keywords\n",
    "# this function return a dictionary with keys as fileid and values as tfidfs\n",
    "# arguments are all the dictionary of text as values and file id as keys and query keywords as a list\n",
    "\n",
    "def get_tfidf(corpus, keywords):\n",
    "    \n",
    "    file_ids = list(corpus.keys())\n",
    "    texts = corpus.values()\n",
    "    \n",
    "    # convert all documents to vectors based on tfidfs of each terms\n",
    "    vectorizer = TfidfVectorizer(lowercase= True, stop_words='english', preprocessor=preprocess)\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    features = vectorizer.get_feature_names()\n",
    "    vectors = X.toarray()\n",
    "    \n",
    "    #retrieve tfidf correspoding to a keyword in the query from the vectors of each document\n",
    "    #sum all tfidf corresponding to keywords in the query for all documents\n",
    "    #save the values in a dictioary and return it\n",
    "    tfidfs = {}\n",
    "    for i in range(len(file_ids)):\n",
    "        tfidf_total = 0\n",
    "        for j in range(len(keywords)):\n",
    "            # apply pre-processing to the query keywords\n",
    "            keyword = keywords[j].lower()\n",
    "            keyword = lemmatizer.lemmatize(keyword)\n",
    "            keyword = lemmatizer.lemmatize(keyword, pos='a')\n",
    "            if keyword in features:\n",
    "                ind = features.index(keyword)\n",
    "                tfidf_total = tfidf_total+vectors[i,ind]\n",
    "        tfidfs[file_ids[i]] = tfidf_total\n",
    "    return tfidfs\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d530c5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for retrieving relevant documents\n",
    "# arguments are a dictionary of texts as values and keys as fileids and numer of keywords in query\n",
    "\n",
    "def get_relevant_docs_by_tfidf(corpus,num_query_keywords):\n",
    "    \n",
    "    # input all the keywords\n",
    "    print(f'\\nPlease enter {num_query_keywords} keywords:')\n",
    "    keywords = []\n",
    "    for i in range(num_query_keywords):\n",
    "        word = input()\n",
    "        keywords.append(word)\n",
    "        \n",
    "    # get tfidfs of each documents corresponding to query keywords\n",
    "    #dictionary with keys as file ids and values as tfidf scores\n",
    "    tfidfs = get_tfidf(corpus,keywords)\n",
    "    \n",
    "    # sort the the tfidfs in descending order \n",
    "    sorted_tfidfs = sorted(tfidfs.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    best_5_docs = sorted_tfidfs[:5] \n",
    "    \n",
    "    # print best 5 ranked documents\n",
    "    print('\\nRelevant documents:')\n",
    "    for doc in best_5_docs:\n",
    "        print(doc[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45598cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get file ids from the inaugural corpus\n",
    "# save all documents in a dictionary with key as file ids and corresponding texts as values\n",
    "file_ids = inaugural.fileids()\n",
    "corpus = {}\n",
    "for i in range(len(file_ids)):\n",
    "    text = inaugural.raw(fileids=file_ids[i])\n",
    "    corpus[file_ids[i]] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "21a414fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please enter 2 keywords:\n",
      "freedom\n",
      "jobs\n",
      "\n",
      "Relevant documents:\n",
      "2005-Bush.txt\n",
      "2013-Obama.txt\n",
      "1957-Eisenhower.txt\n",
      "2017-Trump.txt\n",
      "1985-Reagan.txt\n",
      "\n",
      "Please enter 2 keywords:\n",
      "slavery\n",
      "war\n",
      "\n",
      "Relevant documents:\n",
      "1865-Lincoln.txt\n",
      "1813-Madison.txt\n",
      "1857-Buchanan.txt\n",
      "1821-Monroe.txt\n",
      "1881-Garfield.txt\n",
      "\n",
      "Please enter 2 keywords:\n",
      "liberty\n",
      "slavery\n",
      "\n",
      "Relevant documents:\n",
      "2005-Bush.txt\n",
      "1857-Buchanan.txt\n",
      "1881-Garfield.txt\n",
      "1965-Johnson.txt\n",
      "1841-Harrison.txt\n",
      "\n",
      "Please enter 2 keywords:\n",
      "freedom\n",
      "military\n",
      "\n",
      "Relevant documents:\n",
      "2005-Bush.txt\n",
      "1957-Eisenhower.txt\n",
      "1985-Reagan.txt\n",
      "1953-Eisenhower.txt\n",
      "1949-Truman.txt\n"
     ]
    }
   ],
   "source": [
    "# retrieve by 2 keywords\n",
    "get_relevant_docs_by_tfidf(corpus, num_query_keywords=2)\n",
    "get_relevant_docs_by_tfidf(corpus, num_query_keywords=2)\n",
    "get_relevant_docs_by_tfidf(corpus, num_query_keywords=2)\n",
    "get_relevant_docs_by_tfidf(corpus, num_query_keywords=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "604c4029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please enter 4 keywords:\n",
      "war\n",
      "weapons\n",
      "missile\n",
      "military\n",
      "\n",
      "Relevant documents:\n",
      "1813-Madison.txt\n",
      "1865-Lincoln.txt\n",
      "1985-Reagan.txt\n",
      "1921-Harding.txt\n",
      "1825-Adams.txt\n"
     ]
    }
   ],
   "source": [
    "get_relevant_docs_by_tfidf(corpus,num_query_keywords=4)     # example with 4 keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940fe70c",
   "metadata": {},
   "source": [
    "###### Rank by cosine similiarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "745da0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for tfidf vectorization\n",
    "def get_tfidf_vectors(texts):\n",
    "    vectorizer = TfidfVectorizer(lowercase= True, stop_words='english', preprocessor=preprocess)\n",
    "    vectorizer.fit(texts)\n",
    "    X = vectorizer.transform(texts)\n",
    "    features = vectorizer.get_feature_names()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "56debcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for retrieving relevant documents based on cosine similiarity\n",
    "# arguments are a dictionary of texts as values and keys as fileids and numer of keywords in query\n",
    "\n",
    "def get_docs_by_cosine_similiarity(corpus,num_query_keywords):\n",
    "    \n",
    "    # input all the keywords in query\n",
    "    print(f'\\nPlease enter {num_query_keywords} keywords:')\n",
    "    keywords = []\n",
    "    for i in range(num_query_keywords):\n",
    "        word = input()\n",
    "        word = word.lower()\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        word = lemmatizer.lemmatize(word, pos='a')\n",
    "        keywords.append(word)\n",
    "      \n",
    "\n",
    "    query = ' '.join(keywords)                      # join query keywords\n",
    "    file_ids = list(corpus.keys())                  # get file ids\n",
    "    texts = list(corpus.values())                   # get documents\n",
    "    texts.append(query)                             # add query to documents for vectorization\n",
    "    \n",
    "        \n",
    "    \n",
    "    X = get_tfidf_vectors(texts)                   # get tfidf vectors\n",
    "    query_vector = X[-1, :]                        # retrieve query vector\n",
    "    document_vector = X[:-1,:]                     # retrieve document vectors\n",
    "\n",
    "    \n",
    "    # calculate cosine similiarity between documents and query\n",
    "    # save values in a dictionary with keys as file ids\n",
    "    fileid_similiarity = {}\n",
    "    for i in range(len(file_ids)):\n",
    "        similiarity = cosine_similarity(document_vector[i,:],query_vector)\n",
    "        fileid_similiarity[file_ids[i]] = similiarity[0][0]\n",
    "    \n",
    "    \n",
    "    # sort cosine similiarity in descending order  and retrive best five docs\n",
    "    sorted_similiarity = sorted(fileid_similiarity.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    best_5_docs = sorted_similiarity[:5] \n",
    "    \n",
    "    # print best 5 ranked documents\n",
    "    print('\\nRelevant documents:')\n",
    "    for doc in best_5_docs:\n",
    "        print(doc[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4109ce8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please enter 2 keywords:\n",
      "freedom\n",
      "jobs\n",
      "\n",
      "Relevant documents:\n",
      "2005-Bush.txt\n",
      "2017-Trump.txt\n",
      "2013-Obama.txt\n",
      "2009-Obama.txt\n",
      "1981-Reagan.txt\n",
      "\n",
      "Please enter 2 keywords:\n",
      "slavery\n",
      "war\n",
      "\n",
      "Relevant documents:\n",
      "1865-Lincoln.txt\n",
      "1857-Buchanan.txt\n",
      "1813-Madison.txt\n",
      "1881-Garfield.txt\n",
      "1861-Lincoln.txt\n",
      "\n",
      "Please enter 2 keywords:\n",
      "liberty\n",
      "slavery\n",
      "\n",
      "Relevant documents:\n",
      "1857-Buchanan.txt\n",
      "2005-Bush.txt\n",
      "1881-Garfield.txt\n",
      "1861-Lincoln.txt\n",
      "1889-Harrison.txt\n",
      "\n",
      "Please enter 2 keywords:\n",
      "freedom\n",
      "military\n",
      "\n",
      "Relevant documents:\n",
      "2005-Bush.txt\n",
      "1957-Eisenhower.txt\n",
      "1985-Reagan.txt\n",
      "1953-Eisenhower.txt\n",
      "1949-Truman.txt\n"
     ]
    }
   ],
   "source": [
    "get_docs_by_cosine_similiarity(corpus,num_query_keywords=2)\n",
    "get_docs_by_cosine_similiarity(corpus,num_query_keywords=2)\n",
    "get_docs_by_cosine_similiarity(corpus,num_query_keywords=2)\n",
    "get_docs_by_cosine_similiarity(corpus,num_query_keywords=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da349531",
   "metadata": {},
   "source": [
    "#### Model -2: Binary Independance Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12dd3026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#count vectorizer with binary true, only detect presence of word\n",
    "vectorizer1 = CountVectorizer(stop_words='english',lowercase= True,binary=True,preprocessor=preprocess) \n",
    "#count vectorizer with binary true, count tf in the document\n",
    "vectorizer2 = CountVectorizer(stop_words='english',lowercase= True,binary=False,preprocessor=preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e55436",
   "metadata": {},
   "source": [
    "###### RSV weight of a term t\n",
    "\\begin{equation}\n",
    "\\ W_t = log(\\dfrac{p_t}{1-p_t}) + log(\\dfrac{1-u_t}{u_t})\n",
    "\\ where, p_t = \\dfrac{DF_t}{N+0.5},\n",
    "\\ \\dfrac{1-u_t}{u_t} \\approx \\dfrac{N}{DF_t}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77aedb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating rsv weight of each term\n",
    "# arguments are binary vectors of each documents, feature vector and Number of documents as N\n",
    "\n",
    "def calculate_rsv_weight(X, features, N):\n",
    "    term_rsv = {}\n",
    "    for i in range(len(features)):\n",
    "        DF = sum(X[:,i])                                 #DF of a term sum of column corresponding to the term\n",
    "        p = DF/(N+0.5)\n",
    "        rsv = log(N/DF) + log(p/(1-p))\n",
    "        term_rsv[features[i]] = rsv\n",
    "    \n",
    "    return term_rsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b94b945",
   "metadata": {},
   "source": [
    "###### Relevance score of a document for a query\n",
    "\n",
    "\\begin{equation}\n",
    "\\ rel(D/Q) = \\sum_{t \\in Q} W_t\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4726f8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs_by_bim(corpus,num_query_keywords):\n",
    "    \n",
    "    # input all the keywords in query\\\n",
    "    # preprocess the keywords\n",
    "    print(f'\\nPlease enter {num_query_keywords} keywords:')\n",
    "    keywords = []\n",
    "    for i in range(num_query_keywords):\n",
    "        word = input()\n",
    "        word = word.lower()\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        word = lemmatizer.lemmatize(word, pos='a')\n",
    "        keywords.append(word)\n",
    "        \n",
    "        \n",
    "    texts = corpus.values()\n",
    "    file_ids = list(corpus.keys())\n",
    "    X = vectorizer1.fit_transform(texts)                          # binary vectorize the documents\n",
    "    features = vectorizer1.get_feature_names()\n",
    "    X = X.toarray()\n",
    "    N = len(corpus)\n",
    "    \n",
    "    term_rsv = calculate_rsv_weight(X, features, N)              # calculate rsv of each terms\n",
    "    id_rsv = {} \n",
    "    \n",
    "    # calculate relevance of each document by summing all Wt corresponding to the terms in the query\n",
    "    for i in range(len(file_ids)):\n",
    "        rsv_total = 0\n",
    "        for word in keywords:\n",
    "            if word in features:                             # check keyword in the feature vector\n",
    "                index = features.index(word)\n",
    "                if X[i,index] > 0:                           # check word is in the document\n",
    "                    rsv = term_rsv[word]\n",
    "                    rsv_total = rsv_total+rsv\n",
    "                    \n",
    "        id_rsv[file_ids[i]] = rsv_total                     # save rsv score to the dictionary\n",
    "    \n",
    "    # sort rsv scores and retrieve 5 relevant documents\n",
    "    sorted_rsv = sorted(id_rsv.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    best_5_docs = sorted_rsv[:5] \n",
    "    \n",
    "    # print best 5 ranked documents\n",
    "    print('\\nRelevant documents:')\n",
    "    for doc in best_5_docs:\n",
    "        print(doc[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14e77c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please enter 2 keywords:\n",
      "freedom\n",
      "jobs\n",
      "\n",
      "Relevant documents:\n",
      "1981-Reagan.txt\n",
      "1989-Bush.txt\n",
      "1993-Clinton.txt\n",
      "1997-Clinton.txt\n",
      "2009-Obama.txt\n",
      "\n",
      "Please enter 2 keywords:\n",
      "slavery\n",
      "war\n",
      "\n",
      "Relevant documents:\n",
      "1837-VanBuren.txt\n",
      "1857-Buchanan.txt\n",
      "1861-Lincoln.txt\n",
      "1865-Lincoln.txt\n",
      "1881-Garfield.txt\n",
      "\n",
      "Please enter 2 keywords:\n",
      "liberty\n",
      "slavery\n",
      "\n",
      "Relevant documents:\n",
      "1837-VanBuren.txt\n",
      "1857-Buchanan.txt\n",
      "1861-Lincoln.txt\n",
      "1881-Garfield.txt\n",
      "1889-Harrison.txt\n",
      "\n",
      "Please enter 2 keywords:\n",
      "freedom\n",
      "military\n",
      "\n",
      "Relevant documents:\n",
      "1801-Jefferson.txt\n",
      "1809-Madison.txt\n",
      "1825-Adams.txt\n",
      "1829-Jackson.txt\n",
      "1845-Polk.txt\n"
     ]
    }
   ],
   "source": [
    "get_docs_by_bim(corpus,num_query_keywords=2)\n",
    "get_docs_by_bim(corpus,num_query_keywords=2)\n",
    "get_docs_by_bim(corpus,num_query_keywords=2)\n",
    "get_docs_by_bim(corpus,num_query_keywords=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830af677",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# by binary independance model more documents may have same score, its ignoring TF component\n",
    "# extension to binary independance model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2594e0",
   "metadata": {},
   "source": [
    "###### Two poisson model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f050160a",
   "metadata": {},
   "source": [
    "###### Assumption: All documents having almost similiar lengths\n",
    "###### Relevance score of a document for a query\n",
    "\n",
    "\\begin{equation}\n",
    "\\ rel(D/Q) = \\sum_{t \\in Q} \\dfrac{DF_t(1+k)}{DF_t+k}\\times W_t, where 1 \\leq k <2\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f3150de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs_by_poisson(corpus,num_query_keywords):\n",
    "    \n",
    "    # input all the keywords in query\n",
    "    # preprocess the keywords\n",
    "    print(f'\\nPlease enter {num_query_keywords} keywords:')\n",
    "    keywords = []\n",
    "    for i in range(num_query_keywords):\n",
    "        word = input()\n",
    "        word = word.lower()\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        word = lemmatizer.lemmatize(word, pos='a')\n",
    "        keywords.append(word)\n",
    "        \n",
    "        \n",
    "    texts = corpus.values()\n",
    "    file_ids = list(corpus.keys())\n",
    "    X1 = vectorizer1.fit_transform(texts)            # binary vectorise\n",
    "    X2 = vectorizer2.fit_transform(texts)            # count vectorise\n",
    "    X1 = X1.toarray()\n",
    "    X2 = X2.toarray()\n",
    "    features = vectorizer.get_feature_names()\n",
    "    N = len(corpus)\n",
    "    k = 1\n",
    "    \n",
    "    term_rsv = calculate_rsv_weight(X1, features, N)   # get rsv score \n",
    "    id_tfrsv = {} \n",
    "    \n",
    "    for i in range(len(file_ids)):\n",
    "        tf_rsv_total = 0\n",
    "        for word in keywords:\n",
    "            if word in features:\n",
    "                index = features.index(word)\n",
    "                if X1[i,index] > 0:                           # check word is in the document\n",
    "                    rsv = term_rsv[word]\n",
    "                    tf = X2[i,index]                         # get term frequency\n",
    "                    tf_rsv = (tf*(k+1)/(tf+k))*rsv           # calculate relavence of a term\n",
    "                    tf_rsv_total = tf_rsv_total+tf_rsv       # sum all rrelevance score corresponding to all keyword in query\n",
    "        id_tfrsv[file_ids[i]] = tf_rsv_total\n",
    "    \n",
    "    # sort based on relevance score and get best 5 documents\n",
    "    sorted_tfrsv = sorted(id_tfrsv.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    best_5_docs = sorted_tfrsv[:5] \n",
    "    \n",
    "    # print best 5 ranked documents\n",
    "    print('\\nRelevant documents:')\n",
    "    for doc in best_5_docs:\n",
    "        print(doc[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "72836f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please enter 2 keywords:\n",
      "freedom\n",
      "jobs\n",
      "\n",
      "Relevant documents:\n",
      "2013-Obama.txt\n",
      "1981-Reagan.txt\n",
      "2005-Bush.txt\n",
      "1949-Truman.txt\n",
      "1985-Reagan.txt\n",
      "\n",
      "Please enter 2 keywords:\n",
      "slavery\n",
      "war\n",
      "\n",
      "Relevant documents:\n",
      "1865-Lincoln.txt\n",
      "1857-Buchanan.txt\n",
      "1881-Garfield.txt\n",
      "1821-Monroe.txt\n",
      "1813-Madison.txt\n",
      "\n",
      "Please enter 2 keywords:\n",
      "liberty\n",
      "slavery\n",
      "\n",
      "Relevant documents:\n",
      "2005-Bush.txt\n",
      "1841-Harrison.txt\n",
      "1881-Garfield.txt\n",
      "1857-Buchanan.txt\n",
      "1901-McKinley.txt\n",
      "\n",
      "Please enter 2 keywords:\n",
      "freedom\n",
      "military\n",
      "\n",
      "Relevant documents:\n",
      "1957-Eisenhower.txt\n",
      "1953-Eisenhower.txt\n",
      "1825-Adams.txt\n",
      "1921-Harding.txt\n",
      "1949-Truman.txt\n"
     ]
    }
   ],
   "source": [
    "get_docs_by_poisson(corpus,num_query_keywords=2)\n",
    "get_docs_by_poisson(corpus,num_query_keywords=2)\n",
    "get_docs_by_poisson(corpus,num_query_keywords=2)\n",
    "get_docs_by_poisson(corpus,num_query_keywords=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72bafc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f670177b",
   "metadata": {},
   "source": [
    "#### 3. Custom Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b468a40",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\ tf_{t,d} = log(1+f_{t,d})\n",
    "\\\\idf_t = \\dfrac{N}{1+DF_t}+1\n",
    "\\\\ rel(D/Q) = \\sum_{t \\in Q} tf_{t,d} \\times idf_t\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ae0b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating idf of each term\n",
    "# arguments are binary vectors of each documents, feature vector and Number of documents as N\n",
    "\n",
    "def get_ifd(X,features, N):\n",
    "    term_idf = {}\n",
    "    for i in range(len(features)):\n",
    "        DF = sum(X[:,i])                                 #DF of a term sum of column corresponding to the term\n",
    "        idf = log(N/(1+DF)) + 1\n",
    "        term_idf[features[i]] = idf\n",
    "    \n",
    "    return term_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5b7068d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs_by_custom_model(corpus,num_query_keywords):\n",
    "    \n",
    "    # input all the keywords in query\n",
    "    # preprocess the keywords\n",
    "    print(f'\\nPlease enter {num_query_keywords} keywords:')\n",
    "    keywords = []\n",
    "    for i in range(num_query_keywords):\n",
    "        word = input()\n",
    "        word = word.lower()\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        word = lemmatizer.lemmatize(word, pos='a')\n",
    "        keywords.append(word)\n",
    "        \n",
    "        \n",
    "    texts = corpus.values()\n",
    "    file_ids = list(corpus.keys())\n",
    "    X1 = vectorizer1.fit_transform(texts)            # binary vectorise\n",
    "    X2 = vectorizer2.fit_transform(texts)            # count vectorise\n",
    "    X1 = X1.toarray()\n",
    "    X2 = X2.toarray()\n",
    "    features = vectorizer.get_feature_names()\n",
    "    N = len(corpus)\n",
    "    \n",
    "    term_idf = calculate_rsv_weight(X1, features, N)   # get rsv score \n",
    "    docid_tfidf = {} \n",
    "    \n",
    "    for i in range(len(file_ids)):\n",
    "        tfidf_total = 0\n",
    "        for word in keywords:\n",
    "            if word in features:\n",
    "                index = features.index(word)\n",
    "                if X1[i,index] > 0:                           # check word is in the document\n",
    "                    idf = term_idf[word]\n",
    "                    f = X2[i,index]                          # get frequency of term\n",
    "                    tf = log(1+f)\n",
    "                    tfidf = tf*idf                         # calculate tfidf\n",
    "                    tfidf_total = tfidf_total+tfidf       # sum all tfidf corresponding to all keyword in query\n",
    "        docid_tfidf[file_ids[i]] = tfidf_total\n",
    "    \n",
    "    # sort based on relevance score and get best 5 documents\n",
    "    sorted_tfidf = sorted(docid_tfidf.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    best_5_docs = sorted_tfidf[:5] \n",
    "    \n",
    "    # print best 5 ranked documents\n",
    "    print('\\nRelevant documents:')\n",
    "    for doc in best_5_docs:\n",
    "        print(doc[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8431a974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please enter 2 keywords:\n",
      "freedom\n",
      "jobs\n",
      "\n",
      "Relevant documents:\n",
      "2005-Bush.txt\n",
      "1949-Truman.txt\n",
      "1985-Reagan.txt\n",
      "1957-Eisenhower.txt\n",
      "1953-Eisenhower.txt\n",
      "\n",
      "Please enter 2 keywords:\n",
      "slavery\n",
      "war\n",
      "\n",
      "Relevant documents:\n",
      "1821-Monroe.txt\n",
      "1813-Madison.txt\n",
      "1865-Lincoln.txt\n",
      "1817-Monroe.txt\n",
      "1921-Harding.txt\n",
      "\n",
      "Please enter 2 keywords:\n",
      "liberty\n",
      "slavery\n",
      "\n",
      "Relevant documents:\n",
      "1841-Harrison.txt\n",
      "2005-Bush.txt\n",
      "1881-Garfield.txt\n",
      "1901-McKinley.txt\n",
      "1949-Truman.txt\n",
      "\n",
      "Please enter 2 keywords:\n",
      "freedom\n",
      "military\n",
      "\n",
      "Relevant documents:\n",
      "2005-Bush.txt\n",
      "1957-Eisenhower.txt\n",
      "1949-Truman.txt\n",
      "1985-Reagan.txt\n",
      "1953-Eisenhower.txt\n"
     ]
    }
   ],
   "source": [
    "get_docs_by_custom_model(corpus,num_query_keywords=2)\n",
    "get_docs_by_custom_model(corpus,num_query_keywords=2)\n",
    "get_docs_by_custom_model(corpus,num_query_keywords=2)\n",
    "get_docs_by_custom_model(corpus,num_query_keywords=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb85654d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please enter 4 keywords:\n",
      "military\n",
      "freedom\n",
      "missile\n",
      "war\n",
      "\n",
      "Relevant documents:\n",
      "1921-Harding.txt\n",
      "1825-Adams.txt\n",
      "1953-Eisenhower.txt\n",
      "1901-McKinley.txt\n",
      "1949-Truman.txt\n"
     ]
    }
   ],
   "source": [
    "get_docs_by_custom_model(corpus,num_query_keywords=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "349b914b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please enter 4 keywords:\n",
      "military\n",
      "war\n",
      "weapons\n",
      "missile\n",
      "\n",
      "Relevant documents:\n",
      "1813-Madison.txt\n",
      "1921-Harding.txt\n",
      "1821-Monroe.txt\n",
      "1817-Monroe.txt\n",
      "1865-Lincoln.txt\n"
     ]
    }
   ],
   "source": [
    "get_docs_by_custom_model(corpus,num_query_keywords=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491f7c2c",
   "metadata": {},
   "source": [
    "###### Comparing All models for a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0839fee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF model\n",
      "\n",
      "Please enter 3 keywords:\n",
      "freedom\n",
      "slavery\n",
      "war\n",
      "\n",
      "Relevant documents:\n",
      "2005-Bush.txt\n",
      "1865-Lincoln.txt\n",
      "1813-Madison.txt\n",
      "1881-Garfield.txt\n",
      "1953-Eisenhower.txt\n",
      "\n",
      "Binary Independance Model\n",
      "\n",
      "Please enter 3 keywords:\n",
      "freedom\n",
      "slavery\n",
      "war\n",
      "\n",
      "Relevant documents:\n",
      "1881-Garfield.txt\n",
      "1889-Harrison.txt\n",
      "1909-Taft.txt\n",
      "1953-Eisenhower.txt\n",
      "1997-Clinton.txt\n",
      "\n",
      "Custom Model\n",
      "\n",
      "Please enter 3 keywords:\n",
      "freedom\n",
      "slavery\n",
      "war\n",
      "\n",
      "Relevant documents:\n",
      "1921-Harding.txt\n",
      "1953-Eisenhower.txt\n",
      "1881-Garfield.txt\n",
      "1825-Adams.txt\n",
      "1949-Truman.txt\n"
     ]
    }
   ],
   "source": [
    "print('TF-IDF model')\n",
    "get_relevant_docs_by_tfidf(corpus,num_query_keywords=3) \n",
    "print('\\nBinary Independance Model')\n",
    "get_docs_by_bim(corpus,num_query_keywords=3)\n",
    "print('\\nCustom Model')\n",
    "get_docs_by_custom_model(corpus,num_query_keywords=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a16c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for give set of keywords in the 5 retrieved documents two documents are common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "88997b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF model\n",
      "\n",
      "Please enter 2 keywords:\n",
      "freedom\n",
      "jobs\n",
      "\n",
      "Relevant documents:\n",
      "2005-Bush.txt\n",
      "2013-Obama.txt\n",
      "1957-Eisenhower.txt\n",
      "2017-Trump.txt\n",
      "2009-Obama.txt\n",
      "\n",
      "Binary Independance Model\n",
      "\n",
      "Please enter 2 keywords:\n",
      "freedom\n",
      "jobs\n",
      "\n",
      "Relevant documents:\n",
      "1981-Reagan.txt\n",
      "1989-Bush.txt\n",
      "1993-Clinton.txt\n",
      "1997-Clinton.txt\n",
      "2009-Obama.txt\n",
      "\n",
      "Custom Model\n",
      "\n",
      "Please enter 2 keywords:\n",
      "freedom\n",
      "jobs\n",
      "\n",
      "Relevant documents:\n",
      "2005-Bush.txt\n",
      "1949-Truman.txt\n",
      "1985-Reagan.txt\n",
      "1957-Eisenhower.txt\n",
      "1953-Eisenhower.txt\n"
     ]
    }
   ],
   "source": [
    "print('TF-IDF model')\n",
    "get_relevant_docs_by_tfidf(corpus,num_query_keywords=2) \n",
    "print('\\nBinary Independance Model')\n",
    "get_docs_by_bim(corpus,num_query_keywords=2)\n",
    "print('\\nCustom Model')\n",
    "get_docs_by_custom_model(corpus,num_query_keywords=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57120730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for give set of keywords in the 5 retrieved documents two documents are common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dea30315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF model\n",
      "\n",
      "Please enter 4 keywords:\n",
      "weapons\n",
      "missile\n",
      "war\n",
      "military\n",
      "\n",
      "Relevant documents:\n",
      "1813-Madison.txt\n",
      "1865-Lincoln.txt\n",
      "1985-Reagan.txt\n",
      "1921-Harding.txt\n",
      "1825-Adams.txt\n",
      "\n",
      "Binary Independance Model\n",
      "\n",
      "Please enter 4 keywords:\n",
      "weapons\n",
      "military\n",
      "war\n",
      "missile\n",
      "\n",
      "Relevant documents:\n",
      "1985-Reagan.txt\n",
      "1949-Truman.txt\n",
      "1801-Jefferson.txt\n",
      "1809-Madison.txt\n",
      "1813-Madison.txt\n",
      "\n",
      "Custom Model\n",
      "\n",
      "Please enter 4 keywords:\n",
      "weapons\n",
      "military\n",
      "war\n",
      "missile\n",
      "\n",
      "Relevant documents:\n",
      "1813-Madison.txt\n",
      "1921-Harding.txt\n",
      "1821-Monroe.txt\n",
      "1817-Monroe.txt\n",
      "1865-Lincoln.txt\n"
     ]
    }
   ],
   "source": [
    "print('TF-IDF model')\n",
    "get_relevant_docs_by_tfidf(corpus,num_query_keywords=4) \n",
    "print('\\nBinary Independance Model')\n",
    "get_docs_by_bim(corpus,num_query_keywords=4)\n",
    "print('\\nCustom Model')\n",
    "get_docs_by_custom_model(corpus,num_query_keywords=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d89196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for give set of keywords both custom model(log nomalised TF-IDf) and TF-IDF retrieved almost similiar documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "694fd8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF model\n",
      "\n",
      "Please enter 2 keywords:\n",
      "slavery\n",
      "war\n",
      "\n",
      "Relevant documents:\n",
      "1865-Lincoln.txt\n",
      "1813-Madison.txt\n",
      "1857-Buchanan.txt\n",
      "1821-Monroe.txt\n",
      "1881-Garfield.txt\n",
      "\n",
      "Binary Independance Model\n",
      "\n",
      "Please enter 2 keywords:\n",
      "slavery\n",
      "war\n",
      "\n",
      "Relevant documents:\n",
      "1837-VanBuren.txt\n",
      "1857-Buchanan.txt\n",
      "1861-Lincoln.txt\n",
      "1865-Lincoln.txt\n",
      "1881-Garfield.txt\n",
      "\n",
      "Custom Model\n",
      "\n",
      "Please enter 2 keywords:\n",
      "slavery\n",
      "war\n",
      "\n",
      "Relevant documents:\n",
      "1821-Monroe.txt\n",
      "1813-Madison.txt\n",
      "1865-Lincoln.txt\n",
      "1817-Monroe.txt\n",
      "1921-Harding.txt\n"
     ]
    }
   ],
   "source": [
    "print('TF-IDF model')\n",
    "get_relevant_docs_by_tfidf(corpus,num_query_keywords=2) \n",
    "print('\\nBinary Independance Model')\n",
    "get_docs_by_bim(corpus,num_query_keywords=2)\n",
    "print('\\nCustom Model')\n",
    "get_docs_by_custom_model(corpus,num_query_keywords=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9bb185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# three documents retrieved almost similiar results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5c994692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF model\n",
      "\n",
      "Please enter 2 keywords:\n",
      "freedom\n",
      "military\n",
      "\n",
      "Relevant documents:\n",
      "2005-Bush.txt\n",
      "1957-Eisenhower.txt\n",
      "1949-Truman.txt\n",
      "1985-Reagan.txt\n",
      "1953-Eisenhower.txt\n",
      "\n",
      "Binary Independance Model\n",
      "\n",
      "Please enter 2 keywords:\n",
      "freedom\n",
      "military\n",
      "\n",
      "Relevant documents:\n",
      "1801-Jefferson.txt\n",
      "1809-Madison.txt\n",
      "1825-Adams.txt\n",
      "1829-Jackson.txt\n",
      "1845-Polk.txt\n",
      "\n",
      "Custom Model\n",
      "\n",
      "Please enter 2 keywords:\n",
      "freedom \n",
      "military\n",
      "\n",
      "Relevant documents:\n",
      "1825-Adams.txt\n",
      "1857-Buchanan.txt\n",
      "1921-Harding.txt\n",
      "1829-Jackson.txt\n",
      "1833-Jackson.txt\n"
     ]
    }
   ],
   "source": [
    "print('TF-IDF model')\n",
    "get_relevant_docs_by_tfidf(corpus,num_query_keywords=2) \n",
    "print('\\nBinary Independance Model')\n",
    "get_docs_by_bim(corpus,num_query_keywords=2)\n",
    "print('\\nCustom Model')\n",
    "get_docs_by_custom_model(corpus,num_query_keywords=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec9a34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No common documents found in all three models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
